import os
import time
import tensorflow as tf
import numpy as np
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.optimizers import RMSprop, Adam, SGD
from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger
from tensorflow.keras.utils import to_categorical, plot_model
from tensorflow.keras.applications import ResNet50V2

epochs = 30
verbose = 1
num_classes = 10
batch_size = 128

(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# Normalize data.
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255
x_train_mean = np.mean(x_train, axis=0)
x_train -= x_train_mean
x_test -= x_train_mean

y_train = to_categorical(y_train, num_classes)
y_test = to_categorical(y_test, num_classes)

model = ResNet50V2(input_shape=x_train.shape[1:],
    weights=None, classes=num_classes)

model.compile(
    loss='categorical_crossentropy',
    optimizer=SGD(),
    metrics=['accuracy'])
plot_model(model, to_file='model_resnet_plot.png',
    show_shapes=True, show_layer_names=True)

# # model.summary()
# print(len(model.layers), "layers")
# print("batch_size:", batch_size)
# print(x_train.shape)

# # Prepare model model saving directory.
# save_dir = os.path.join(os.getcwd(), 'saved_models')
# os.makedirs(save_dir, exist_ok=True)
# model_type = "{}epochs".format(epochs)
# model_name = 'cifar10_{0}_model.{{epoch:03d}}.h5'.format(model_type)

# filepath = os.path.join(save_dir, model_name)

# # Prepare callbacks for model saving and for learning rate adjustment.
# checkpoint = ModelCheckpoint(
#     filepath=filepath,
#     monitor='val_accuracy',
#     verbose=1,
#     save_best_only=True)

# logger = CSVLogger(f'cifar10_{model_type}.csv')

# callbacks = [checkpoint, logger]

# t0 = time.time()
# print('Using real-time data augmentation.')
# # This will do preprocessing and realtime data augmentation:
# datagen = ImageDataGenerator(
#     # set input mean to 0 over the dataset
#     featurewise_center=False,
#     # set each sample mean to 0
#     samplewise_center=False,
#     # divide inputs by std of dataset
#     featurewise_std_normalization=False,
#     # divide each input by its std
#     samplewise_std_normalization=False,
#     # apply ZCA whitening
#     zca_whitening=False,
#     # randomly rotate images in the range (deg 0 to 180)
#     rotation_range=0,
#     # randomly shift images horizontally
#     width_shift_range=0.1,
#     # randomly shift images vertically
#     height_shift_range=0.1,
#     # randomly flip images
#     horizontal_flip=True,
#     # randomly flip images
#     vertical_flip=False)

# # Compute quantities required for featurewise normalization
# # (std, mean, and principal components if ZCA whitening is applied).
# datagen.fit(x_train)

# # Fit the model on the batches generated by datagen.flow().
# model.fit(
#     datagen.flow(x_train, y_train, batch_size=batch_size),
#     validation_data=(x_test, y_test),
#     epochs=epochs, verbose=verbose, workers=4,
#     callbacks=callbacks,
#     steps_per_epoch=x_train.shape[0]//batch_size)

# print("done training in {}s".format(time.time()-t0))

# # Score trained model.
# scores = model.evaluate(x_test, y_test,
#     verbose=verbose, batch_size=batch_size)
# print('Test loss:', scores[0])
# print('Test accuracy:', scores[1])